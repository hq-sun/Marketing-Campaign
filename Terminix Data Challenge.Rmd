---
title: "Terminix Data Challenge"
author: "Heqing Sun"
date: "6/18/2020"
output: html_document
---

## Environment Setup
```{r}
setwd('/Users/sun/Documents/GitHub/Terminix-Data-Challenge')
# getwd()

library(tidyverse)
library(janitor)
library(magrittr)
library(mlr)
library(readxl)
library(rjson)
library(utils)
library(RCurl)
library(censusapi)
library(sf)
library(tigris)
library(devtools)
library(zeallot)
library(httr)
library(plyr)
library(jsonlite)
library(tigris)
library(corrplot)
library(DataExplorer)
library(naniar)
library(corrplot)
library(reshape2)
library(flexclust)
library(data.table)
library(mltools)
```

## Access the cvs file
```{r}
data <- read.csv(file = 'data/raw/dataset.csv')
## 23651 obs, 46 vars
```

## Data Exploration
```{r}
summarizeColumns(data) %>%
  select(name,type,na,mean,median,min,max)%>%
  mutate(percent_miss=(na/nrow(data))*100)
## Maximum missing percentage is 16.4%
## There are four percentage columns ranging from 0 to 1000: MTA6200A_PCT, MTA7430_AVG, MTF6200A_PCT, MTF6280A_PCT

names(data[sapply(data, is.factor)])
## 5 categorical variables: CANCEL_REASON, CUSTOMER_CANCEL_REASON, IS_ELIGIBLE_FOR_EVERGREEN, Mosaic_Z4, REN220_SALES_CHANNEL
```

## Plot histograms for four problematic pct variables
```{r}
data_problematic <- data %>% select(MTA6200A_PCT, MTA7430_AVG, MTF6200A_PCT, MTF6280A_PCT)

data_problematic %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = "free") +
  geom_histogram()
```

## Deal with Categorical Variables
```{r}
count(data, 'CANCEL_REASON')  ## 24 levels
count(data, 'CUSTOMER_CANCEL_REASON')  ## 59 levels
count(data, 'IS_ELIGIBLE_FOR_EVERGREEN')  ## 2 levels
count(data, 'Mosaic_Z4')  ## 72 levels
count(data, 'REN220_SALES_CHANNEL')  ## 3 levels

# Replace N, Y with 0, 1 in IS_ELIGIBLE_FOR_EVERGREEN column
data$IS_ELIGIBLE_FOR_EVERGREEN <- gsub('N', 0, data$IS_ELIGIBLE_FOR_EVERGREEN)
data$IS_ELIGIBLE_FOR_EVERGREEN <- gsub('Y', 1, data$IS_ELIGIBLE_FOR_EVERGREEN)
data$IS_ELIGIBLE_FOR_EVERGREEN <- as.numeric(as.character(data$IS_ELIGIBLE_FOR_EVERGREEN))

# Use one-hot encoding for other categorical variables
data_1h <- one_hot(as.data.table(data))
# names(data_1h)
## 23651 obs, 200 vars

# Check if any columns are non-numeric
data_1h %>% select_if(negate(is.numeric))
## 0 rows
```

## Deal with duplicates
```{r}
data_1h_dup <- data_1h[duplicated(data_1h$Ref1),]

# get all duplicated obs
data_1h_dup_Ref1 <- data_1h_dup$Ref1
dupe <- data_1h[is.element(data_1h$Ref1, data_1h_dup_Ref1),]
## 46 obs in total, and 19 of them are unique Ref1.
## But some of them have different Ref1, all other variables are the same.

# Remove duplicated rows based on Ref1
data_1h_no_dup_Ref1 <- data_1h %>% distinct(Ref1, .keep_all = TRUE)

# Convert Ref1 - record key to the index
data_1h_no_dup_Ref1 <- data_1h_no_dup_Ref1 %>%
     remove_rownames() %>%
     column_to_rownames(var = 'Ref1')
## 23624 obs, 199 vars
## data_1h_no_dup_Ref1: no dupliates in Ref1, but still some vars have exact same all other variables values except Ref1
```

## Data Partitioning - Split training and validation sets
```{r}
smp_size <- floor(0.75 * nrow(data_1h_no_dup_Ref1))

# set the seed to make the partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(data_1h_no_dup_Ref1)), size = smp_size)

train <- data_1h_no_dup_Ref1[train_ind, ]
test <- data_1h_no_dup_Ref1[-train_ind, ]

X_train <- train %>% select (-sale)
X_test <- test %>% select (-sale)
y_train <- train %>% select (sale)
y_test <- test %>% select (sale)
```

## Pre-processing the data
```{r}
# Use complete cases for feature selection
train_cc <- train %>%
  filter(complete.cases(.)) 

summarizeColumns(train_cc) %>%
  select(name,type,na,mean,median,min,max)%>%
  mutate(percent_miss=(na/nrow(train_cc))*100)
## No missing values, but some variables are all 0 at this complete case scenario

# Remove columns that are all 0s
train_cc_no0 <- train_cc[, colSums(train_cc != 0) > 0]
```

## Boruta
```{r}
# Perform Boruta search
library(Boruta)
boruta_output <- Boruta(sale ~ ., data=na.omit(train), doTrace=0)  

# Get significant variables including tentatives
boruta_signif <- getSelectedAttributes(boruta_output, withTentative = TRUE)
print(boruta_signif)
## 58 variables are selected
```

```{r}
# Do a tentative rough fix
roughFixMod <- TentativeRoughFix(boruta_output)
boruta_signif <- getSelectedAttributes(roughFixMod)
print(boruta_signif)
## 52 variables are selected
```

```{r}
# Variable Importance Scores
imps <- attStats(roughFixMod)
imps2 = imps[imps$decision != 'Rejected', c('meanImp', 'decision')]
boruta_df <- head(imps2[order(-imps2$meanImp), ], 10)  # descending sort
boruta_vars <- row.names(boruta_df)
print(boruta_vars)
# [1] "CUSTOMER_CANCEL_REASON_OTHER" "BASE_PRICE"                   "BCC5520_AVG"
# [4] "ILN6200A_PCT"                 "REV7430_AVG"                  "OPTIONS_PRICE"
# [7] "BCC5421_AVG"                  "ALL6200A_PCT"                 "ALL6280A_PCT"
# [10] "ILN7430_AVG"

# Plot variable importance
plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance")  
```

## Variable Importance from Machine Learning Algorithms - super fast
```{r}
# Train an rpart model and compute variable importance.
library(caret)
set.seed(100)
# colnames(train) <- make.names(colnames(train))

# Convert target variable to the factor
train$sale = as.factor(train$sale)
rPartMod <- train(sale ~ ., data=na.omit(train), method="rpart")
rpartImp <- varImp(rPartMod)
print(rpartImp)
# CUSTOMER_CANCEL_REASON_OTHER	BASE_PRICE		OPTIONS_PRICE			REN220_SALES_CHANNEL_DTC.NON.TPV			
# REN220_SALES_CHANNEL_DTC.TPV			CUSTOMER_CANCEL_REASON_CM		
# Mosaic_Z4_L41			Mosaic_Z4_J35			CUSTOMER_CANCEL_REASON_TPEXCDENLS		REV6200A_PCT
## Some of these variables have importance value as 0, so probably not that reliable.
```







## Rank Features By Importance
```{r}
set.seed(123)
# load the library
library(mlbench)
library(caret)

# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
model <- train(sale~., data=train_cc_no0, method="lvq", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(model, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)
```


## Correlation Matrix
```{r}
# Remove redudant features

set.seed(7)
# load the library
library(mlbench)
library(caret)

# calculate correlation matrix
correlationMatrix <- cor(X_train_1h_cc_no0[,])
# summarize the correlation matrix
print(correlationMatrix)
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5)
# print indexes of highly correlated attributes
print(highlyCorrelated)

```




