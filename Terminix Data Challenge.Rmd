---
title: "Terminix Data Challenge"
author: "Heqing Sun"
date: "6/18/2020"
output: html_document
---

## Environment Setup
```{r}
setwd('/Users/sun/Documents/GitHub/Terminix-Data-Challenge')
# getwd()

library(tidyverse)
library(janitor)
library(magrittr)
library(mlr)
library(readxl)
library(rjson)
library(utils)
library(RCurl)
library(censusapi)
library(sf)
library(tigris)
library(devtools)
library(zeallot)
library(httr)
library(plyr)
library(jsonlite)
library(tigris)
library(corrplot)
library(DataExplorer)
library(naniar)
library(corrplot)
library(reshape2)
library(flexclust)
library(data.table)
library(mltools)
```

## Access the cvs file
```{r}
data <- read.csv(file = 'data/raw/dataset.csv')
## 23651 obs, 46 vars
```

## Data Exploration
```{r}
summarizeColumns(data) %>%
  select(name,type,na,mean,median,min,max)%>%
  mutate(percent_miss=(na/nrow(data))*100)
## Maximum missing percentage is 16.4%
## There are four percentage columns ranging from 0 to 1000: MTA6200A_PCT, MTA7430_AVG, MTF6200A_PCT, MTF6280A_PCT

names(data[sapply(data, is.factor)])
## 5 categorical variables: CANCEL_REASON, CUSTOMER_CANCEL_REASON, IS_ELIGIBLE_FOR_EVERGREEN, Mosaic_Z4, REN220_SALES_CHANNEL
```

# Deal with four problematic pct variables
```{r}
# Plot histograms
m.data_problematic <- data %>% select(MTA6200A_PCT, MTA7430_AVG, MTF6200A_PCT, MTF6280A_PCT)

m.data_problematic %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = "free") +
  geom_histogram()

# Change their range to 1-100
data$MTA6200A_PCT <- data$MTA6200A_PCT/10
data$MTA7430_AVG <- data$MTA7430_AVG/10
data$MTF6200A_PCT <- data$MTF6200A_PCT/10
data$MTF6280A_PCT <- data$MTF6280A_PCT/10

data %>% select(MTA6200A_PCT, MTA7430_AVG, MTF6200A_PCT, MTF6280A_PCT) %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = "free") +
  geom_histogram()
```

## Deal with Categorical Variables
```{r}
count(data, 'CANCEL_REASON')  ## 24 levels
count(data, 'CUSTOMER_CANCEL_REASON')  ## 59 levels
count(data, 'IS_ELIGIBLE_FOR_EVERGREEN')  ## 2 levels
count(data, 'Mosaic_Z4')  ## 72 levels
count(data, 'REN220_SALES_CHANNEL')  ## 3 levels

# Replace N, Y with 0, 1 in IS_ELIGIBLE_FOR_EVERGREEN column
data$IS_ELIGIBLE_FOR_EVERGREEN <- gsub('N', 0, data$IS_ELIGIBLE_FOR_EVERGREEN)
data$IS_ELIGIBLE_FOR_EVERGREEN <- gsub('Y', 1, data$IS_ELIGIBLE_FOR_EVERGREEN)
data$IS_ELIGIBLE_FOR_EVERGREEN <- as.numeric(as.character(data$IS_ELIGIBLE_FOR_EVERGREEN))

# Use one-hot encoding for other categorical variables
m.data_1h <- one_hot(as.data.table(data))
## 23651 obs, 200 vars

# Check if any columns are non-numeric
m.data_1h %>% select_if(negate(is.numeric))
## 0 rows
```

## Deal with duplicates
```{r}
m.data_1h_dup <- m.data_1h[duplicated(m.data_1h$Ref1),]

# Remove duplicated rows based on Ref1
m.data_1h_no_dup_Ref1 <- m.data_1h %>% distinct(Ref1, .keep_all = TRUE)

# Convert Ref1 - record key to the index
data_no_dup <- m.data_1h_no_dup_Ref1 %>%
     remove_rownames() %>%
     column_to_rownames(var = 'Ref1')
## 23624 obs, 199 vars
## data_1h_no_dup_Ref1: no dupliates in Ref1, but still some vars have exact same all other variables values except Ref1 - no worries right now

# Save to RDS
saveRDS(data_no_dup, './data/clean/data_no_dup.rds')

# Clean up environment
rm(list = ls(pattern = "^m."))
```

# Check if target variable is imbalanced
```{r}
table(data_no_dup$sale)
## 5.4 : 1 imbalanced dataset
```

## Data Partitioning - Split training and validation sets for selecting important features
```{r}
m.smp_size <- floor(0.75 * nrow(data_no_dup))

# set the seed to make the partition reproducible
set.seed(123)
m.train_ind <- sample(seq_len(nrow(data_no_dup)), size = m.smp_size)

m.train <- data_no_dup[m.train_ind, ]
m.test <- data_no_dup[-m.train_ind, ]
```

## Boruta
```{r}
# Perform Boruta search
library(Boruta)
boruta_output <- Boruta(sale ~ ., data=na.omit(m.train), doTrace=0)  ## took 20 min to run

# Get significant variables including tentatives
boruta_signif <- getSelectedAttributes(boruta_output, withTentative = TRUE)
print(boruta_signif)
## 57 variables are selected
```

```{r}
# Do a tentative rough fix
roughFixMod <- TentativeRoughFix(boruta_output)
boruta_signif <- getSelectedAttributes(roughFixMod)
print(boruta_signif)
## 54 variables are selected
```

```{r}
# Variable Importance Scores
imps <- attStats(roughFixMod)
imps2 = imps[imps$decision != 'Rejected', c('meanImp', 'decision')]
boruta_df <- head(imps2[order(-imps2$meanImp), ], 10)  # descending sort
boruta_vars <- row.names(boruta_df)
print(boruta_vars)
# [1] "CUSTOMER_CANCEL_REASON_OTHER" "BASE_PRICE"                   "BCC5520_AVG"
# [4] "REV7430_AVG"                 "ILN6200A_PCT"                  "OPTIONS_PRICE"
# [7] "BCC5421_AVG"                  "ALL6200A_PCT"                 "ALL6280A_PCT"
# [10] "ILN7430_AVG"

# Plot variable importance
plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance")  
```

## Regularized Random Forest (RRF) algorithm - took too long, did not run it successfully
```{r}
# Train an RRF model and compute variable importance.
set.seed(100)
rrfMod <- train(sale ~ ., data=na.omit(train), method="RRF")

rrfImp <- varImp(rrfMod, scale=F)
rrfImp

plot(rrfImp, top = 20, main='Variable Importance')
```



## Subest Data Partitioning - Split training and validation sets (only 10 features)
```{r}
m.smp_size <- floor(0.75 * nrow(data_no_dup))

# set the seed to make the partition reproducible
set.seed(123)
m.train_ind <- sample(seq_len(nrow(data_no_dup)), size = m.smp_size)

data_sub <- data_no_dup %>% select(sale, all_of(boruta_vars)) ## 23624 obs, 11 vars
data_sub_noTarget <- data_no_dup %>% select(all_of(boruta_vars)) ## 23624 obs, 10 vars
# boruta_vars = c("CUSTOMER_CANCEL_REASON_OTHER", "BASE_PRICE", "BCC5520_AVG", "REV7430_AVG", "ILN6200A_PCT", "OPTIONS_PRICE", "BCC5421_AVG", "ALL6200A_PCT", "ALL6280A_PCT", "ILN7430_AVG")

# Save to RDS
saveRDS(data_sub, './data/clean/data_sub.rds')
saveRDS(data_sub_noTarget, './data/clean/data_sub_noTarget.rds')

m.train <- data_sub[m.train_ind, ]
m.test <- data_sub[-m.train_ind, ]

# Save them to csv
write.csv(x=m.train, file="./data/clean/train_sub.csv")
write.csv(x=m.test, file="./data/clean/test_sub.csv")

# X_train <- train %>% select (-sale)
# X_test <- test %>% select (-sale)
y_train <- m.train %>% select (sale)
y_test <- m.test %>% select (sale)
```

## Add Missing Value Indicator
```{r}
summarizeColumns(data_sub_noTarget) %>%
  select(name,type,na,mean,median,min,max)%>%
  mutate(percent_miss=(na/nrow(data_sub_noTarget))*100)

# Create missing indicators, but some of them will be all 0
for (var in names(data_sub_noTarget)) {
    data_sub_noTarget[[paste0(var, "_missing")]] <- ifelse(is.na(data_sub_noTarget[[var]]), 1, 0)
  }

# Filter missing indicators
m.mi <- data_sub_noTarget %>% 
  select(ends_with("_missing"))

# Filter non-zero missing indicators
m.mi_non0 = m.mi[colSums(Filter(is.numeric, m.mi)) != 0]
names(m.mi_non0)
all(m.mi_non0$BCC5520_AVG_missing == m.mi_non0$ILN6200A_PCT_missing)
all(m.mi_non0$BCC5520_AVG_missing == m.mi_non0$REV7430_AVG_missing)
all(m.mi_non0$BCC5520_AVG_missing == m.mi_non0$BCC5421_AVG_missing)
all(m.mi_non0$BCC5520_AVG_missing == m.mi_non0$ALL6200A_PCT_missing)
all(m.mi_non0$BCC5520_AVG_missing == m.mi_non0$ALL6280A_PCT_missing)
all(m.mi_non0$BCC5520_AVG_missing == m.mi_non0$ILN7430_AVG_missing)
## These seven indicators have the exact same distribution, so only need one to represent all other 6

m.mi_good <- m.mi_non0 %>% select(BCC5520_AVG_missing, OPTIONS_PRICE_missing)

# Combine the original non-imputed data with the non-zero missing indicators
data_sub_noTarget <- data_no_dup %>% select(all_of(boruta_vars))
data_sub_noTarget_mi <- cbind(data_sub_noTarget, m.mi_good)
## 23624 obs, 12 vars

# Save to RDS
saveRDS(data_sub_noTarget_mi, './data/clean/data_sub_noTarget_mi.rds')
```

## Imputing the NA using the MICE method in training and test data separately
```{r}
# Split train test data
m.train_noTarget <- data_sub_noTarget_mi[m.train_ind, ]
m.test_noTarget <- data_sub_noTarget_mi[-m.train_ind, ]

library(mice)
m.train_imputed <- mice(data=m.train_noTarget, m=5, method="cart", maxit=10, where = is.na(m.train_noTarget))

m.test_imputed <- mice(data=m.test_noTarget, m=5, method="cart", maxit=10, where = is.na(m.test_noTarget))

# picking one of the inerations to be work on
m.train_complete <- complete(m.train_imputed, 5)
m.test_complete <- complete(m.test_imputed, 5)

# check if still NAs after the imputation
sum(is.na(m.train_complete)) ## 0
sum(is.na(m.test_complete)) ## 0

write.csv(x=m.train_complete, file="./data/clean/train_imputed_noTarget.csv")
write.csv(x=m.test_complete, file="./data/clean/test_imputed_noTarget.csv")
```

## Logistic regression model
```{r}
# Add Target column
train_data <- cbind(m.train_complete, sale = y_train$sale)
test_data <- cbind(m.test_complete, sale = y_test$sale)

write.csv(x=train_data, file="./data/clean/X_train.csv")
write.csv(x=test_data, file="./data/clean/X_test.csv")

# Add weights column to address the imbalanced data issue
# X_train$wt <- with(X_train, ifelse(sale==0, 20, 1))

# Model
# Log1 = glm(sale~., data = X_train,  family = binomial('logit'), maxit = 100, weights = wt)
# glm.fit: fitted probabilities numerically 0 or 1 occurred
# summary(Log1)

```

## Modeling the original unbalanced data
```{r}
library(caret)
set.seed(123)
# Convert target variable to factor
train_data$sale <- as.factor(train_data$sale)
test_data$sale <- as.factor(test_data$sale)

model_lr <- caret::train(sale ~ .,
                         data = train_data,
                         method = "glm",
                         preProcess = c("scale", "center"),
                         trControl = trainControl(method = "repeatedcv", 
                                                  number = 10, 
                                                  repeats = 10, 
                                                  verboseIter = FALSE))

final <- data.frame(actual = test_data$sale,
                    predict(model_lr, newdata = test_data, type = "prob"))
# X_train$wt <- with(X_train, ifelse(sale==0, 20, 1))
final$predict <- ifelse(final$X0 > 0.5, 0, 1)
cm_original <- confusionMatrix(factor(final$predict), factor(final$actual), positive = "1")
cm_original
#           Reference
# Prediction    0    1
#          0 4953  857
#          1   36   60
```

# Under-sampling
```{r}
ctrl <- trainControl(method = "repeatedcv", 
                     number = 10, 
                     repeats = 10, 
                     verboseIter = FALSE,
                     sampling = "down")

set.seed(123)
model_lg_under <- caret::train(sale ~ .,
                         data = train_data,
                         method = "glm",
                         preProcess = c("scale", "center"),
                         trControl = ctrl)

final_under <- data.frame(actual = test_data$sale,
                    predict(model_lg_under, newdata = test_data, type = "prob"))


final_under$predict <- ifelse(final_under$X0 > 0.5, 0, 1)
cm_under <- confusionMatrix(factor(final_under$predict), factor(final_under$actual), positive = "1")
cm_under
#           Reference
# Prediction    0    1
#          0 3248  427
#          1 1741  490
```

## Over-sampling
```{r}
ctrl <- trainControl(method = "repeatedcv", 
                     number = 10, 
                     repeats = 10, 
                     verboseIter = FALSE,
                     sampling = "up")

set.seed(123)
model_lg_over <- caret::train(sale ~ .,
                         data = train_data,
                         method = "glm",
                         preProcess = c("scale", "center"),
                         trControl = ctrl)

final_over <- data.frame(actual = test_data$sale,
                          predict(model_lg_over, newdata = test_data, type = "prob"))

final_over$predict <- ifelse(final_over$X0 > 0.5, 0, 1)
cm_over <- confusionMatrix(factor(final_over$predict), factor(final_over$actual), positive = "1")
cm_over
#           Reference
# Prediction    0    1
#          0 3272  433
#          1 1717  484

```

## ROSE
```{r}
ctrl <- trainControl(method = "repeatedcv", 
                     number = 10, 
                     repeats = 10, 
                     verboseIter = FALSE,
                     sampling = "rose")

set.seed(123)
model_lg_rose <- caret::train(sale ~ .,
                              data = train_data,
                              method = "glm",
                              preProcess = c("scale", "center"),
                              trControl = ctrl)

final_rose <- data.frame(actual = test_data$sale,
                          predict(model_lg_rose, newdata = test_data, type = "prob"))

final_rose$predict <- ifelse(final_rose$X0 > 0.5, 0, 1)
cm_rose <- confusionMatrix(factor(final_rose$predict), factor(final_rose$actual), positive = "1")
cm_rose
# Prediction    0    1
#          0 3151  430
#          1 1838  487
```

## SMOTE (Synthetic Minority Oversampling Technique)
```{r}
ctrl <- trainControl(method = "repeatedcv", 
                     number = 10, 
                     repeats = 10, 
                     verboseIter = FALSE,
                     sampling = "smote")

set.seed(123)

model_lg_smote <- caret::train(sale ~ .,
                              data = train_data,
                              method = "glm",
                              preProcess = c("scale", "center"),
                              trControl = ctrl)

final_smote <- data.frame(actual = test_data$sale,
                          predict(model_lg_smote, newdata = test_data, type = "prob"))

final_smote$predict <- ifelse(final_smote$X0 > 0.5, 0, 1)
cm_smote <- confusionMatrix(factor(final_smote$predict), factor(final_smote$actual), positive = "1")
cm_smote
#           Reference
# Prediction    0    1
#          0 4353  685
#          1  636  232
```

## SMOTE - Random Forest -didn't run successfully
```{r}
ctrl <- trainControl(method = "repeatedcv", 
                     number = 10, 
                     repeats = 10, 
                     verboseIter = FALSE,
                     sampling = "smote")

set.seed(123)
model_rf_smote <- caret::train(sale ~ .,
                              data = train_data,
                              method = "rf",
                              preProcess = c("scale", "center"),
                              trControl = ctrl)


final_rf_smote <- data.frame(actual = test_data$sale,
                          predict(model_rf_smote, newdata = test_data, type = "prob"))

final_rf_smote$predict <- ifelse(final_rf_smote$X0 > 0.5, 0, 1)

cm_rf_smote <- confusionMatrix(factor(final_rf_smote$predict), factor(final_rf_smote$actual))
cm_rf_smote
```


## Compare the predictions of all these models
```{r}
models <- list(original = model_lr,
                       under = model_lg_under,
                       over = model_lg_over,
                       smote = model_lg_smote,
                       rose = model_lg_rose)

resampling <- resamples(models)
bwplot(resampling)

## SMOTE have the comparable accuuracy to original, but way higher specificity and sensitivity
```

```{r}
comparison <- data.frame(model = names(models),
                         Sensitivity = rep(NA, length(models)),
                         Specificity = rep(NA, length(models)),
                         Precision = rep(NA, length(models)),
                         Recall = rep(NA, length(models)),
                         F1 = rep(NA, length(models)))

comparison[comparison$model == "original", ] <- filter(comparison, model == "original") %>%
    mutate(Sensitivity = cm_original$byClass["Sensitivity"],
           Specificity = cm_original$byClass["Specificity"],
           Precision = cm_original$byClass["Precision"],
           Recall = cm_original$byClass["Recall"],
           F1 = cm_original$byClass["F1"])

comparison[comparison$model == "under", ] <- filter(comparison, model == "under") %>%
    mutate(Sensitivity = cm_under$byClass["Sensitivity"],
           Specificity = cm_under$byClass["Specificity"],
           Precision = cm_under$byClass["Precision"],
           Recall = cm_under$byClass["Recall"],
           F1 = cm_under$byClass["F1"])

comparison[comparison$model == "over", ] <- filter(comparison, model == "over") %>%
    mutate(Sensitivity = cm_over$byClass["Sensitivity"],
           Specificity = cm_over$byClass["Specificity"],
           Precision = cm_over$byClass["Precision"],
           Recall = cm_over$byClass["Recall"],
           F1 = cm_over$byClass["F1"])

comparison[comparison$model == "smote", ] <- filter(comparison, model == "smote") %>%
    mutate(Sensitivity = cm_smote$byClass["Sensitivity"],
           Specificity = cm_smote$byClass["Specificity"],
           Precision = cm_smote$byClass["Precision"],
           Recall = cm_smote$byClass["Recall"],
           F1 = cm_smote$byClass["F1"])

comparison[comparison$model == "rose", ] <- filter(comparison, model == "rose") %>%
    mutate(Sensitivity = cm_rose$byClass["Sensitivity"],
           Specificity = cm_rose$byClass["Specificity"],
           Precision = cm_rose$byClass["Precision"],
           Recall = cm_rose$byClass["Recall"],
           F1 = cm_rose$byClass["F1"])

## FOR LOOP not working here
# for (name in names(models)) {
#   model <- get(paste0("cm_", name))
# 
#   comparison[comparison$model == name, ] <- filter(comparison, model == name) %>%
#     mutate(Sensitivity = model$byClass["Sensitivity"],
#            Specificity = model$byClass["Specificity"],
#            Precision = model$byClass["Precision"],
#            Recall = model$byClass["Recall"],
#            F1 = model$byClass["F1"])
# }

comparison %>%
  gather(x, y, Sensitivity:F1) %>%
  ggplot(aes(x = x, y = y, color = model)) +
    geom_jitter(width = 0.2, alpha = 0.5, size = 3)
```


```{r}
# Let's predict outcome on Training dataset
PredictTrain <- predict(Log1, type = "response")
summary(PredictTrain)

# This computes the average prediction for each of the two outcomes
tapply(PredictTrain, train_complete_withTarget$sale, mean)
```

```{r}
# Build confusion matrix with a threshold value of 0.5
threshold_0.5 <- table(train_complete_withTarget$sale, PredictTrain > 0.5)
threshold_0.5

# Accuracy
accuracy_0.5 <- round(sum(diag(threshold_0.5))/sum(threshold_0.5),2)
sprintf("Accuracy is %s",accuracy_0.5)
## Accuracy is 0.85

# Mis-classification error rate
MC_0.5 <- 1-accuracy_0.5
sprintf("Mis-classification error is %s",MC_0.5)
## Mis-classification error is 0.15

sensitivity0.5 <- round(192/(2602+192),2)
specificity0.5 <- round(14827/(14827+97),2)
sprintf("Sensitivity at 0.5 threshold: %s", sensitivity0.5)
## Sensitivity at 0.5 threshold: 0.07
sprintf("Specificity at 0.5 threshold: %s", specificity0.5)
## Specificity at 0.5 threshold: 0.99
```

## Generate ROC Curves
```{r}
library(ROCR)

ROCRpred = prediction(PredictTrain, train_complete_withTarget$sale)
ROCRperf = performance(ROCRpred, "tpr", "fpr")

# Adding threshold labels
plot(ROCRperf, colorize=TRUE, print.cutoffs.at = seq(0,1,0.1), text.adj = c(-0.2, 1.7))
abline(a=0, b=1)

auc_train <- round(as.numeric(performance(ROCRpred, "auc")@y.values),2)
legend(.8, .2, auc_train, title = "AUC", cex=1)
```

## Making predictions on test set
```{r}
test_complete_withTarget <- cbind(m.test_sub_noTarget_mi, sale = y_test$sale)

PredictTest <- predict(Log1, type = "response", newdata = test_complete_withTarget)

# Convert probabilities to values using the below

## Based on ROC curve above, selected a threshold of 0.5
test_tab <- table(test_complete_withTarget$sale, PredictTest > 0.5)
test_tab

accuracy_test <- round(sum(diag(test_tab))/sum(test_tab),2)
sprintf("Accuracy on test set is %s", accuracy_test)
## Accuracy on test set is 0.85

# Compute test set AUC
ROCRPredTest = prediction(PredictTest, test_complete_withTarget$sale)
auc = round(as.numeric(performance(ROCRPredTest, "auc")@y.values),2)
auc

```





```{r}
library(MASS)
library(caret)
# Make predictions
predictions <- Log1 %>% predict(m.test_sub_noTarget_mi)
predictions
# Model performance
data.frame(
  RMSE = RMSE(predictions, y_test),
  R2 = R2(predictions, y_test)
)
```


## Check Multicollinearity
```{r}
car::vif(Log1) ## not a problem
```



## Correlation Matrix
```{r}
x.res <- cor(train_sub[sapply(train_sub, is.numeric)], use='pairwise')

corrplot(x.res, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)
```



```{r}
# Remove redudant features
set.seed(7)
# load the library
library(mlbench)
library(caret)

# calculate correlation matrix
correlationMatrix <- cor(X_train_1h_cc_no0[,])
# summarize the correlation matrix
print(correlationMatrix)
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5)
# print indexes of highly correlated attributes
print(highlyCorrelated)

```




